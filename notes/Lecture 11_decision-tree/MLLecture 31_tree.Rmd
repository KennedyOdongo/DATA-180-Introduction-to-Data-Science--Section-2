---
title: "Introduction to Supervised Learning <br>Part 2: Decision Tree"
author: "Eren Bilen <br> Dickinson College"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: inline
---
```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(base_color = "#1c5253",colors = c(
  red = "#f34213",
  purple = "#3e2f5b",
  orange = "#ff8811",
  green = "#136f63",
  white = "#FFFFFF"
))
```


# Basics of Decision Trees

- Decision trees can be applied to both regression and classification problems.
 - regression tree: when you predict a value on a continuous scale
 - classification tree: when you try to predict a categorical variable, e.g., 0-1 binary
 
- More flexible than linear regression, *i.e. it can easily fit linear and non-linear!*
 - At the cost of interpretability (instead of a mathematical model, we will have bunch of if-else conditions)
 - You don't care about interpretation, if your only goal is to predict anyway

- We will cover regression trees (they use numeric, continuous outcome), but classification trees are very similar (binary outcome). 
- Check out the ISLR textbook (link on Github) Chapter 8, page 327 for more.

---
#Regression Trees

In order to motivate regression trees, we begin with a simple example: Predicting a baseball player's salary

- Our motivation is to to predict a baseball player’s `Salary` based on `Years` (the number of years that he has played in the major leagues) and `Hits` (the number of hits he made in the previous year). 

- Some data wrangling: We first remove observations that are missing `Salary` values, and log-transform Salary so that its distribution has more of a typical bell-shape. 

- Note: `Salary` is measured in thousands of dollars.

---
#Prediction of baseball player's salary

```{r, echo=FALSE, out.width="30%", fig.cap="Simple decision tree",fig.align='center'}
knitr::include_graphics("examples/reg-partition-tree.png")
```

---
# What does the tree mean?

- The tree represents a series of splits starting at the top of the tree. (root)

- The first node (root) assigns observations with $Years < 4.5$ to the left branch.

 - The predicted salary for these players is given by the mean response value for the players in the data set (with $Years < 4.5.$)

 - For such players, the mean log salary is 5.107, and so we make a prediction of $e^{5.107}$ dollars (in thousands), i.e. $165,174$.



---
#Prediction of baseball player's salary


```{r, echo=FALSE, out.width="40%", fig.cap="Divided regions",fig.align='center'}
knitr::include_graphics("examples/reg-partition-hitters.png")
```

---
#What do the regions mean?

We can write these regions as the following:

1. $R_1: {X \mid Years < 4.5}$
2. $R_2: {X \mid Years \geq 4.5,  Hits < 117.5}$
3. $R_3: {X \mid Years \geq 4.5,  Hits \geq 117.5}.$

---
#Interpretation of the figure

- `Years` is a significant divider in determining `Salary.` Players with less experience earn lower salaries than more experienced players. 

- Given that a player is less experienced, the number of hits that he made in the previous year seems to play little role in his salary. 

- Among players who have been in the major leagues for 4.5 or more years, the number of hits made in the previous year does affect salary, and players who made more hits last year tend to have higher salaries. 


---
# How do we build a regression tree?


1. We divide the predictor space into a set of possible values for $X_1,\ldots,X_p$ into $J$ distinct and non-overlapping regions, $R_1,\ldots,R_J$.

2. For every observation that falls into the region $R_j$,  we make the following prediction: the mean of the response values for the training observations in $R_j$.

Suppose that in Step 1, we obtain two regions and that the response mean of the training observations in the first region is 10, while the response mean in the second region is 20. Then for a given observation that falls in the first region, we will predict a value of 10, otherwise, we will predict a value of 20.

But how do we actually construct the regions? 

---
# Constructing the regions

- The regions in theory could have any shape. 

- However, we choose to divide the predictor space into high-dimensional rectangles or boxes (for simplicity and ease of interpretation of the resulting predictive model).

Our goal is to find boxes $R_1, \ldots, R_J$ that minimize the SSR (or RSS) given by 
$$RSS = \sum_{j=1}^J \sum_{i \in R_j} (y_i -  \bar{y}_{R_j})^2,$$ where $\bar{y}_{R_j}$ is the mean response for the training observations within the $j$th box. 
---
## Constructing the regions (continued)

1)  We first select the predictor $X_j$ and the cutpoint $s$ such that splitting the predictor space into the regions ${\{X|X_j<s\}}$ and ${\{X|X_j{\ge}s}\}$ leads to the greatest possible reduction in RSS.

2)  Repeat the process looking for the best predictor and best cutpoint to split data further (i.e., split one of the 2 previously identified regions - not the entire predictor space) minimizing the RSS within each of the resulting regions.

3)  Continue until a stopping criterion is reached, e.g., each region contains at least five observations.


---
## But...

-   The previous method may result in a tree that **overfits** the data. Why?

-   Tree is too leafy (complex). SSR is expected to go down for more complex trees.

-   So we will *prune* the tree efficiently to get a less complex tree. More on this to come.

---
#Application with Hitters data set

Let's return to growing a regression tree for the Hitters dataset. 

Recall that we use the Hitters data set to predict a baseball players `Salary` based on `Years` (the number of years that he has played in the major leagues) and `Hits` (the number of hits that he made in the previous year).

There are several R packages for regression trees; the easiest one is called, simply, `tree()`.


---
#Application with Hitters data set

```{r, warning=FALSE,message=FALSE}
library(ISLR)
library(tree)
library(tidyverse)
attach(Hitters)
# remove NA values
Hitters <- na.omit(Hitters)
Salary <- na.omit(Salary)
# put salary on log scale, split sample to test/train, and fit reg tree
Hitters <- Hitters %>% mutate(logsalary = log(Salary))
trainingloc <- sample(1:nrow(Hitters), 0.8*nrow(Hitters))
training <- Hitters[trainingloc, ]

testloc <- setdiff(1:nrow(Hitters), trainingloc) # finds the "different" rows
test <- Hitters[testloc, ]
treefit <- tree(log(Salary) ~ Years + Hits, data=training)
```

---
Observe the summary of the above regression tree and plot the regression tree.

```{r}
summary(treefit)
```

- There are 8 terminal nodes or leaves of the tree.
- Here "deviance" is just the MSE.

---
# Plot tree 

```{r}
plot(treefit)
text(treefit,cex=0.75) # add labels
```

---
```{r}
preds <- data.frame('salary_pred' = predict(treefit, test))
preds %>% head
check <- data.frame('actual' = Hitters[rownames(preds),]$logsalary, 
                    'pred' = preds[,1], 'hits' = Hitters[rownames(preds),]$Hits)
check %>% head
```


---
```{r,fig.height=6}
plot(check$hits, check$actual, col='red')
par(new=TRUE)
plot(check$hits, check$pred,col='blue',ann=FALSE, axes=FALSE)
```


---
```{r,fig.height=6}
# compare with linear regression
lsq <- lm(training$logsalary ~ training$Hits)
preds_lsq <- lsq$coefficients[1] + lsq$coefficients[2] * test$Hits
plot(check$hits, check$actual,col='red')
par(new=TRUE)
plot(check$hits, preds_lsq,col='blue',ann=FALSE, axes=FALSE)
```

---
# Compare MSEs
```{r}
sum((check$actual - preds_lsq)^2)/length(check)
sum((check$actual - check$pred)^2)/length(check)
```

---
# How to plot the regions:
```{r, echo=TRUE, fig.height=6}
plot(Years,Hits,pch=16,col='grey', xlab="Years",ylab="Hits")
partition.tree(treefit,ordvars=c("Years","Hits"),add=TRUE)
```


---
#Pruning 

The tree package contains functions `prune.tree()` for pruning trees 

The function `prune.tree()` takes a tree you fit via `tree()`, and evaluates the error of the tree and various prunings of the tree.

**If you ask it for a particular size of tree, it gives you the best pruning of that size. (via the best= option)**

If you don’t ask it for the best tree, it gives an object which shows the number of leaves in the pruned trees, and the error of each one. (This object can be plotted.)

---
#Hitters Data

```{r,fig.height=5}
# train the tree
treefit <- tree(log(Salary) ~ Years + Hits, data=training, minsize=5)
# Sequence of pruned tree sizes/errors
treefit_n = prune.tree(treefit)
plot(treefit_n) # error versus tree size, you can ignore the top axis
```

---
# Get the best pruned tree

```{r, fig.height=4.2}
treefit <- tree(log(Salary) ~ Years + Hits, data=training)
treefit_n = prune.tree(treefit) # returns all possible trees
# get tree scores
alpha = 4 # need to estimate this from data, change to improve SSR on test sample
score = treefit_n$dev + treefit_n$size * alpha
# size of optimal tree
opt_tree = treefit_n$size[which(score == min(score))]
```

---

```{r}
# use the optimal size
treefit_pruned = prune.tree(treefit,best=opt_tree) # where best specifies tree size
# plot the pruned tree
plot(treefit_pruned)
text(treefit_pruned,cex=0.75,digits=3)
```


---
# Application: Decision Tree (binary outcome)

A decision tree application for whether a user sends a like on a social media post or not. (binary outcome!)

```{r cit1a, echo = T}
library(tree)
library(dplyr)
library(grid)
# load data
citdata <- read.delim("https://slcladal.github.io/data/treedata.txt", header = T, sep = "\t") %>%
# convert character strings to factors
dplyr::mutate_if(is.character, factor)
# set.seed
set.seed(111)        
dtree <- tree(LikeUser ~ Age + Gender + Status, data = citdata, split = "gini")
```
---

```{r,message=FALSE, warning=FALSE,out.width="40%"}
# display decision tree
plot(dtree, gp = gpar(fontsize = 8))
# add annotation
text(dtree, pretty = 0, all = F)
```

